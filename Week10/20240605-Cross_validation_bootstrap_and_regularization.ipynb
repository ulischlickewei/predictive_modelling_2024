{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1821d5ab-8d41-4d0d-b05b-d215d1d7830e",
   "metadata": {},
   "source": [
    "## Preparing the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6c8919-3445-43b8-a26c-f0b16e5eb6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from ISLP import load_data\n",
    "from ISLP.models import (ModelSpec as MS,\n",
    "                         summarize,\n",
    "                         poly)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from functools import partial\n",
    "from sklearn.model_selection import \\\n",
    "     (cross_validate,\n",
    "      KFold,\n",
    "      ShuffleSplit)\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import make_scorer\n",
    "from ISLP.models import sklearn_sm\n",
    "\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2c30f4-852b-4370-beb5-71fe23129186",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eab36e5-c629-4ae3-8783-89823f392af7",
   "metadata": {},
   "source": [
    "# Part 1: Case study cross validation\n",
    "\n",
    "(see Exercise 5.4.5)\n",
    "\n",
    "In this case study we use the credit card dataset to predict the probability of default. We will build a logistic regression model and estimate its test error using the validation set approach and the cross-validation approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ed8e43-d25a-4a02-988f-49ce67770870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to load the data\n",
    "Default = load_data('Default')\n",
    "Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9e5715-b491-4b6c-8cb2-e3cd2dfaee6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Default.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9c6317-cf99-49d9-ae60-753f09eb793a",
   "metadata": {},
   "source": [
    "Background information on the dataset can be found [in the documentation](https://islp.readthedocs.io/en/latest/datasets/Default.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a75412-c7de-491e-b9be-d0ec886617b0",
   "metadata": {},
   "source": [
    "## Task 1.1 (completed last week)\n",
    "Fit a logistic regression model that uses `income` and `balance` to predict `default`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727109ed-48b7-4c9e-bc44-8c36b14a3c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = Default.columns.drop(['default','student'])\n",
    "design = MS(predictors).fit(Default)\n",
    "X = design.transform(Default)\n",
    "y = Default.default.map(\n",
    "    {'No': 0,\n",
    "    'Yes': 1}\n",
    ")\n",
    "\n",
    "model = sm.Logit(y,X)\n",
    "results = model.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cc651b-8665-48f2-a7d1-bfadb49d1c94",
   "metadata": {},
   "source": [
    "## Task 1.2 (completed last week)\n",
    "Using the validation set approach, estimate the test error of this model. In order to do this, you must perform the following steps:\n",
    "\n",
    "i. Split the sample set into a training set and a validation set.\n",
    "\n",
    "ii. Fit a multiple logistic regression model using only the training observations.\n",
    "\n",
    "iii. Obtain a prediction of default status for each individual in the validation set by computing the posterior probability of default for that individual, and classifying the individual to the default category if the posterior probability is greater than 0.5.\n",
    "\n",
    "iv. Compute the validation set error, which is the fraction of the observations in the validation set that are misclassified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98a4cf7-6068-4c36-a14b-449001af21a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i. splitting into validation and test set\n",
    "train, val = train_test_split(\n",
    "    Default,\n",
    "    test_size=0.3,\n",
    "    random_state = 42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be22a42-07cc-45da-80e1-83f630369a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ii. fit logistic regression model using only training observations\n",
    "predictors = train.columns.drop(['default','student'])\n",
    "design2 = MS(predictors).fit(train)\n",
    "X2 = design2.transform(train)\n",
    "y2 = train.default.map(\n",
    "    {'No': 0,\n",
    "    'Yes': 1}\n",
    ")\n",
    "\n",
    "model2 = sm.Logit(y2,X2)\n",
    "results2 = model2.fit()\n",
    "results2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe229423-58c8-411b-b3fe-2207f609a52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iii. Prediction for validation set\n",
    "X_val = design2.transform(val)\n",
    "predicted_probs = results2.predict(X_val)\n",
    "predicted_labels = np.where(predicted_probs > 0.5, 'Yes', 'No')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcd0f8c-5650-4e0d-9825-ab562f158a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iv. validation set error\n",
    "true_labels = val.default\n",
    "val_err = 1 - np.mean(predicted_labels == true_labels)\n",
    "val_err"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515624b3-284a-4b56-a120-c979eb72843c",
   "metadata": {},
   "source": [
    "Thus, the test error estimated by the validation set method is $2.7\\%$.\n",
    "Note that this measure overestimates the quality of the model. While only $2.7\\%$ of misclassified observations sounds like a good model, the cell below shows that the model produces quite a high number of false negatives (i.e. defaults which are predicted as non-defaults by the model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7834fc9-861b-40c1-91b3-cb20c10adf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels[true_labels == 'Yes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1871e39e-9c77-48c7-ab88-43628f98bc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ISLP import confusion_table\n",
    "confusion_table(predicted_labels, true_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79e016c-9404-478e-9ca7-dd5b4d405bdf",
   "metadata": {},
   "source": [
    "## Task 1.3 (completed last week)\n",
    "Repeat the process in (b) three times, using three different splits of the observations into a training set and a validation set. Comment on the results obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6678f73-12fb-4cc6-a898-c065e3d0247f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "val_errors = np.zeros(3)\n",
    "for i in range(3):\n",
    "    train, val = train_test_split(\n",
    "        Default,\n",
    "        test_size=0.3\n",
    "    )\n",
    "    predictors = train.columns.drop(['default','student'])\n",
    "    design3 = MS(predictors).fit(train)\n",
    "    X3 = design3.transform(train)\n",
    "    y3 = train.default.map(\n",
    "        {'No': 0,\n",
    "        'Yes': 1}\n",
    "    )\n",
    "\n",
    "    model3 = sm.Logit(y3,X3)\n",
    "    results3 = model3.fit()\n",
    "    X_val = design3.transform(val)\n",
    "    predicted_probs = results2.predict(X_val)\n",
    "    predicted_labels = np.where(predicted_probs > 0.5, 'Yes', 'No')\n",
    "    true_labels = val.default\n",
    "    val_err = 1 - np.mean(predicted_labels == true_labels)\n",
    "    val_errors[i] = val_err\n",
    "val_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aff045b-0b1b-4243-a7fd-f402df04919f",
   "metadata": {},
   "source": [
    "**Comment**: The fraction of observations which are misclassified varies slightly due to the fact that we use different validation sets (no random seed set!) in each of the three iterations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856893a5-2422-4e97-8641-5bbe75504e0b",
   "metadata": {},
   "source": [
    "## Task 1.4\n",
    "Now predict the test error of the model using 10-fold cross-validation. To do so, follow the steps we developed in Part 1b) of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22331be-c6a8-4fb8-82f5-b85b952df421",
   "metadata": {},
   "source": [
    "**Note**: To carry out this task, we need to define our own custom score using [`sklearn.make_scorer()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html) and pass this to the function [`sklearn.cross_validate()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html) by specifying it as an argument to the `scoring` parameter.\n",
    "\n",
    "Our custom scoring function needs to have the signature `score_func(y, y_pred, **kwargs)` with `y` being the true labels and `y_pred` the predicted labels as output by `sm.Logit()` when applying the `predict()` method. Since `statsmodels` outputs probabilities rather than actual labels, we first transform these probabilities  into labels. This is what our scoring function `accuracy_score_sm()` does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5eb851-765d-499d-9cdb-4f9f2f2d26c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define custom scorer which computes accuracy based on probabilities as\n",
    "# output by statsmodels\n",
    "from sklearn.metrics import make_scorer\n",
    "def accuracy_score_sm(y_true, y_pred_prob):\n",
    "    # computes the accuracy for the output of a binary classification model\n",
    "\n",
    "    # inputs:\n",
    "    #    y_true: ground truth labels, encoded as 0,1\n",
    "    #    y_pred_prob: probabilities for label 1 as predicted by the model\n",
    "\n",
    "    # output:\n",
    "    #    percentage of models \n",
    "    ...\n",
    "    return ...\n",
    "    \n",
    "accuracy_sm = make_scorer(accuracy_score_sm)\n",
    "\n",
    "# Step 2: Initialize splitter for cross validation and model\n",
    "cross_val = ...\n",
    "model = ...\n",
    "\n",
    "# Step 3: Define response variable and design matrix for cross validation\n",
    "y = ...\n",
    "predictors = ...\n",
    "X = ...\n",
    "\n",
    "# Step 4: Run cross validation\n",
    "cv_results = cross_validate(model,\n",
    "                            X,\n",
    "                            y,\n",
    "                            scoring = accuracy_sm,\n",
    "                            cv = cross_val)\n",
    "\n",
    "print('Mean accuracy: ', np.mean(cv_results['test_score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d4eeec-0d91-4fb1-8ac1-8e066fedd68d",
   "metadata": {},
   "source": [
    "## Task 1.5\n",
    "Now consider a logistic regression model that predicts the probability of default using income, balance, and a dummy variable for student. Estimate the test error for this model using 10-fold cross-validation. Comment on whether or not including a dummy variable for student leads to a reduction in the test error rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d922248-f09f-49b6-9b39-a873c4883974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Initialize splitter for cross validation and model\n",
    "...\n",
    "\n",
    "# Step 2: Define response variable and design matrix for cross validation\n",
    "...\n",
    "\n",
    "# Step 3: Run cross validation\n",
    "...\n",
    "\n",
    "print('Mean accuracy: ', np.mean(cv_results['test_score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e447a78f-6de3-4181-9741-25192d29db09",
   "metadata": {},
   "source": [
    "*Your interpretation here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29a4c7d-a6e4-4069-90ca-297f6b0be87d",
   "metadata": {},
   "source": [
    "# Part 2: Implementation of bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35bdedf-192c-4e2c-b95e-49897a3bf93d",
   "metadata": {},
   "source": [
    "## Bootstrapping for estimating the accuracy of a statistic\n",
    "### Introducing the dataset\n",
    "We closely follow an example presented in [Computational and Inferential Thinking](https://inferentialthinking.com/chapters/13/3/Confidence_Intervals.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa7a56c-e6f9-4076-9ea7-53ebc6f931bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://drive.google.com/uc?id='\n",
    "file_id = \"15xUDQPqkzKJBoxrafC9iNz4EgFlwbmM_\"\n",
    "births = pd.read_csv(url + file_id)\n",
    "births"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7f388d-e618-4d0d-8d94-990c24227b2a",
   "metadata": {},
   "source": [
    "#### Task 2.1\n",
    "\n",
    "Create a new column `Birth Weight (g)` which contains the birth weight in kg. Use the fact 1oz = 28.3495g for your computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e75b2e6-78cc-4183-a4a5-a6bce400b451",
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fc53d3-3f6d-4562-a0ae-f31ceb497679",
   "metadata": {},
   "source": [
    "Birth weight is an important factor in the health of a newborn infant. Smaller babies tend to need more medical care in their first days than larger newborns. It is therefore helpful to have an estimate of birth weight before the baby is born. One way to do this is to examine the relationship between birth weight and the number of gestational days.\n",
    "\n",
    "A simple measure of this relationship is the ratio of birth weight to the number of gestational days. The table ratios contains the first two columns of baby, as well as a column of the ratios. The first entry in that column was calculated as follows:\n",
    "$$ \\frac{3401.94 \\text{oz}}{284 \\text{ days}} \\approx 11.98 \\text{g} \\text{ per day}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1448b110-b01e-4c2b-ac93-8be4d8cc29f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratios = pd.DataFrame({\n",
    "    'Birth Weight' : births['Birth Weight (g)'],\n",
    "    'Ratio BW:GD' : births['Birth Weight (g)'] / births['Gestational Days']\n",
    "})\n",
    "ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fb3e7f-67f0-4e96-8fe6-b306691328bf",
   "metadata": {},
   "source": [
    "#### Task 2.2\n",
    "\n",
    "Plot a histogram of the ratios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b89d008-c778-4557-82ab-4952daecbf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670cd39b-c836-48a8-9b36-cc5371ac7fd0",
   "metadata": {},
   "source": [
    "#### Task 2.3\n",
    "\n",
    "Compute the median ratio and the maximum ratio in the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f78d01-393f-425d-9362-caddda9aa884",
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b95260-fb79-454b-a48f-4ef74310a13c",
   "metadata": {},
   "source": [
    "### Estimating the variability of the population median\n",
    "We now want to estimate the population median. For this we are going to use the bootstrapping method.\n",
    "We start by reviewing the idea in a graphical manner:\n",
    "![bootstrap.png](bootstrap.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49aac012-f61e-408a-90f2-f3d92cd6f9b7",
   "metadata": {},
   "source": [
    "#### Task 2.4\n",
    "\n",
    "Define a function `one_bootstrap_median` which will bootstrap the sample and return the median ratio in the bootstrapped sample.\n",
    "\n",
    "- To bootstrap the sample use the Pandas.DataFrame method [`sample`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sample.html). *Important*: Make sure to draw a sample of the same length as our original sample and make sure to sample with replacement.\n",
    "- To compute the appropriate quantile, use the Numpy method [`quantile`](https://numpy.org/doc/stable/reference/generated/numpy.quantile.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e82d39d-c8b6-4b13-86ba-7b101d9ba430",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_bootstrap_median():\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cc30de-eb7d-44c7-a76c-eafad735a741",
   "metadata": {},
   "source": [
    "#### Task 2.5\n",
    "\n",
    "Initialize a Numpy vector `bootstrap_medians` with zeros of length 5000 (use the Numpy method [`zeros()`](https://numpy.org/doc/stable/reference/generated/numpy.zeros.html#numpy.zeros). Then fill this vector with 5000 bootstrapped medians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b26b817-6268-4adb-8533-7c052b39841b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_medians = ...\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55b6280-80ba-45fb-a2b4-6815f9296ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax  = plt.subplots(figsize =(8,8))\n",
    "sns.histplot(x = bootstrap_medians, bins=25);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f441ba0-e6c2-451d-a978-5c4d31403195",
   "metadata": {},
   "source": [
    "## Bootstrapping for estimating the accuracy of a Linear Regression Model\n",
    "Now, we discuss how to use bootstrapping in order to assess the variability of the coefficient estimates and predictions from a statistical learning method. As an example, we look at a simple linear regression model based on the `Auto` dataset which predicts the `mpg` variable based on `horsepower`.\n",
    "\n",
    "With the bootstrap method we are going to estimate the distribution of the coefficient for `mpg` in this model and we compare the standard error of this coefficient as estimated by `statsmodels` with our bootstrap estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dde7f67-7dce-42e4-b2ba-e82e1a124955",
   "metadata": {},
   "outputs": [],
   "source": [
    "Auto = load_data('Auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426ef279-f074-4e3b-9961-a52a0e25eeb4",
   "metadata": {},
   "source": [
    "#### Task 2.6 \n",
    "\n",
    "Define a function `one_bootstrap_model_coefficient` which creates a single bootstrap sample from the Auto dataframe, computes a regression model based on the single predictor `horsepower` and returns the model coefficient for `horsepower`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adc4476-6684-4bf1-b83c-69120181b940",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_bootstrap_model_coefficient():\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70fd91b-337b-4341-bb79-41fe29e843c4",
   "metadata": {},
   "source": [
    "#### Task 2.7\n",
    "\n",
    "Initialize a Numpy vector `bootstrap_model_coefficients` with zeros of length 5000 (use the Numpy method [`zeros()`](https://numpy.org/doc/stable/reference/generated/numpy.zeros.html#numpy.zeros). Then fill this vector with 5000 bootstrapped model coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d2e0f1-ca8d-41d0-aa25-4bfe5df057fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_model_coefficients = ...\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900f9da5-8abd-4d97-995b-7c0463013ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax  = plt.subplots(figsize =(8,8))\n",
    "sns.histplot(x = bootstrap_model_coefficients, bins=25);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05c6dd4-3c38-4b7a-b17d-ce83889d3ce7",
   "metadata": {},
   "source": [
    "#### Task 2.8 \n",
    "\n",
    "Estimate the standard error of the model coefficient for `horsepower` and assign it to the variable `standard_error_bootstrap`. Compare to the `sm.OLS()` estimate which should be assigned to the variable `standard_error_bootstrap`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3d00c4-f233-472d-84c7-2332adb8f3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_error_bootstrap = ...\n",
    "\n",
    "# compute statsmodels OLS model using the full dataset\n",
    "...\n",
    "standard_error_statsmodels = ...\n",
    "\n",
    "print('Bootstrapped standard error for model coefficient:', \"{:10.4f}\".format(standard_error_bootstrap))\n",
    "print('Statsmodels OLS standard error estimate for model coefficient:', \"{:10.4f}\".format(standard_error_statsmodels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf526b8-36be-4696-8d8e-fb75fcebe589",
   "metadata": {},
   "source": [
    "# Part 3: Case study bootstrap\n",
    "\n",
    "We continue to consider the use of a logistic regression model to predict the probability of default using income and balance on the `Default` data set. In particular, we will now compute estimates for the standard errors of the income and balance logistic regression coefficients in two different ways: \n",
    "1. using the bootstrap, and \n",
    "2. using the standard formula for computing the standard errors in the sm.GLM() function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ee55e0-f562-469e-b8e6-ad15fc52fda5",
   "metadata": {},
   "source": [
    "## Task 3.1\n",
    "Using the `summarize()` and `sm.GLM()` functions, determine the estimated standard errors for the coefficients associated with `income` and `balance` in a multiple logistic regression model that uses both predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece8e8c9-4be6-425f-8191-043b6452645b",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfb5ce4-e4bd-4076-989a-96542a570bd8",
   "metadata": {},
   "source": [
    "## Task 3.2\n",
    "Following the bootstrap example in Part 2 above, estimate the standard errors of the logistic regression coefficients for income and balance with the bootstrap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bfea0f-cced-4ff9-ade9-85790472f24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define function to compute one bootstrap sample of the model coefficients\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b9bb1f-5ea9-4f5a-9388-ad3ffa0a404d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: \n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfce934-2864-4497-8f54-b9a70885044c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3a: comparison of bootstrap standard errors and standard errors as per statsmodels.Logit() - balance\n",
    "print('Bootstrap estimation of standard error for balance parameter:   ', \n",
    "      '{:6e}'.format(np.std(balance_coefficients))\n",
    ")\n",
    "print('Statsmodels estimation of standard error for balance parameter: ', \n",
    "      '{:6e}'.format(summarize(results).loc['balance','std err'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3456658d-f02f-4318-9831-b33d560f2154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3b: comparison of bootstrap standard errors and standard errors as per statsmodels.Logit() - income\n",
    "print('Bootstrap estimation of standard error for income parameter:   ', \n",
    "      '{:6e}'.format(np.std(income_coefficients))\n",
    ")\n",
    "print('Statsmodels estimation of standard error for income parameter: ', \n",
    "      '{:6e}'.format(summarize(results).loc['income','std err'])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ecc781-c801-4abc-b6d7-d3b0da91068c",
   "metadata": {},
   "source": [
    "## Task 3.3\n",
    "\n",
    "Comment on the estimated standard errors obtained using the `sm.Logit()`/`sm.GLM()` function and using the bootstrap."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d07b5c-8901-4d15-a6fc-57c9cec115d1",
   "metadata": {},
   "source": [
    "*Your comment here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c155972d-68f4-47db-98bb-2acf8022b963",
   "metadata": {},
   "source": [
    "# Part 4: Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d71fd0-814e-47e3-b567-0f91382d8256",
   "metadata": {},
   "source": [
    "In this section we learn how to implement regularization for linear regression models using Ridge and the Lasso formalisms.\n",
    "\n",
    "We look at a [market research project by a pharmaceutical company](https://www.tandfonline.com/doi/abs/10.1080/02664763.2014.994480) (example taken from the textbook [Learning Data Science](https://learningds.org/ch/16/ms_regularization.html#lipovetsky)) by S. Lau, J. Gonzalez and D. Nolan).\n",
    "\n",
    "The objective of the study is to model consumer interest in purchasing a cold sore health-care product. The study authors gather data from 1,023 consumers. Each consumer is asked to rate on a 10-point scale 35 factors according to whether the factor matters to them when considering purchasing a cold sore treatment. They also rate their interest in purchasing the product.\n",
    "\n",
    "We begin by reading in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f276f6-be7c-4ef8-8f2d-f337eaa86665",
   "metadata": {},
   "outputs": [],
   "source": [
    "ma_df = pd.read_csv('market-analysis.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd9c753-76b8-4edf-a1af-e75aee76cac1",
   "metadata": {},
   "source": [
    "The table below lists the 35 factors and provides their correlation to the outcome, their interest in purchasing the product:\n",
    "\n",
    "\n",
    "\n",
    "|  | Corr | Description |  | Corr | Description |\n",
    "| --- | --- | --------- | --- | --- | --------- |\n",
    "| x1  | 0.70 | provides soothing relief | x19 | 0.54 | has a non-messy application |\n",
    "| x2  | 0.58 | moisturizes cold sore blister | x20 | 0.70 | good for any stage of a cold |\n",
    "| x3  | 0.69 | provides long-lasting relief | x21 | 0.49 | easy to apply/take |\n",
    "| x4  | 0.70 | provides fast-acting relief | x22 | 0.52 | package keeps from contamination |\n",
    "| x5 | 0.72 | shortens duration of a cold | x23 | 0.57 | easy to dispense a right amount |\n",
    "| x6  | 0.68 | stops the virus from spreading | x24 | 0.63 | worth the price it costs |\n",
    "| x7 | 0.67| dries up cold sore | x25 | 0.57 | recommended most by pharamacists |\n",
    "| x8 | 0.72 | heals fast | x26 | 0.54 | recommended by doctors |\n",
    "| x9 | 0.72 | penetrates deep | x27 | 0.54 | FDA approved |\n",
    "| x10 | 0.65 | relieves pain | x28 | 0.64 | a brand I trust |\n",
    "| x11 |0.61 | prevents cold | x29 | 0.60 | clinically proven |\n",
    "| x12 | 0.73 | prevents from getting worse | x30 | 0.68 | a brand I would recommend |\n",
    "| x13 | 0.57 | medicated | x31 | 0.74 | an effective treatment |\n",
    "| x14 | 0.61 | prescription strength | x32  |0.37 | portable |\n",
    "| x15 | 0.63 | repairs damaged skin | x33 | 0.37 | discreet packaging |\n",
    "| x16 | 0.67 | blocks virus from spreading | x34 | 0.55 | helps conceal cold sores |\n",
    "| x17 | 0.42 | contains SPF | x35 | 0.63 | absorbs quickly |\n",
    "| x18 | 0.57 | non-irritating | | | |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f31ccb-4035-4499-90a6-de1246bfb204",
   "metadata": {},
   "source": [
    "Based on their labels alone, some of these 35 features appear to measure similar aspects of desirability. We can compute the correlations between the explanatory variables to confirm this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e87e0e-aef2-4e89-bdc1-2e3425fb6e79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ma_df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ca6b7e-7800-4289-830c-dd2ade47b3b9",
   "metadata": {},
   "source": [
    "We observe for example that the last feature `x35` (\"arsorbs quickly\") is highly correlated to `x1` (\"provides soothing relief\"), `x4` (\"provides fast-acting relief\") or `x9` (\"penetrates deep\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4e2474-8d6a-4459-b576-dcdf4e2cd5bb",
   "metadata": {},
   "source": [
    "## Task 4.1\n",
    "\n",
    "Split the data into train and test sets. Use a test set size of 200 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7667898d-c6c9-40de-9570-12f9da7d8c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = ma_df[\"y\"]\n",
    "X = ma_df.drop(columns=[\"y\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=200, shuffle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82c3df3-57f4-4746-8a7a-4070b54322e8",
   "metadata": {},
   "source": [
    "## Task 4.2\n",
    "\n",
    "Standardize the features using the `sklearn.preprocessing.StandardScaler()` (see [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) method. Note that only the predictors need to be scaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241f82af-2eda-457c-86ef-d17ac2016950",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scalerX = StandardScaler().fit(X_train) \n",
    "X_train_scaled = scalerX.transform(X_train)\n",
    "X_test_scaled = scalerX.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71842a82-c7f4-49be-bb60-cc12f078a20c",
   "metadata": {},
   "source": [
    "Run the cell below to check that scaled training data has mean 0 and SD 1 (approximately):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd9b022-0ac7-4c31-8f06-ae5c4a529666",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35acc72a-1bde-4272-9ea7-945ef5fa9b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92088dd1-263c-482d-99d8-89293c633e2b",
   "metadata": {},
   "source": [
    "Note that this is **not** the case for the test data (**why?**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60aca358-b1ec-44a6-9dc6-a2d9a02666bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451f45bd-d02f-4832-af1d-2ed5b9a901f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4930fc5-b1a8-46ee-b8d2-7335c3b0bf9d",
   "metadata": {},
   "source": [
    "## Task 4.3\n",
    "\n",
    "We start by computing an ordinary multiple linear regression model. For consistency with the subsequent tasks we use `sklearn.linear_models.LinearRegression` this time (see [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html).\n",
    "\n",
    "In the following, train a Multiple Linear Regression model on the scaled training data.\n",
    "\n",
    "Compute the model coefficients (using the `coef_` attribute of the trained model) and the mean squared error on the test data (using the function `sklearn.metrics.mean_squared_error()` (see [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html))).\n",
    "\n",
    "*Note*: The design matrix and the results vector are passed only as arguments to the `fit()` method for `sklearn` models. This is different than for `statsmodels` where we passed the data already at the stage of initializing the model. Additionally, the order in which the design matrix and the results vector are passed to a `sklearn`-model is swapped compared to `statsmodels`!\n",
    "\n",
    "Also note: for linear models in `sklearn` we do not need to manually create an `intercept` column as we can specify if we want an intercept to be included using the `fit_intercept` argument when initializing the model. This parameter is set to `True` by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c41e0c3-bea2-4de4-b681-2d4e7ce7e2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "coefficients = model.coef_\n",
    "mse = mean_squared_error(y_test, model.predict(X_test_scaled))\n",
    "\n",
    "print('Multiple Linear Regression model coefficients: ', coefficients)\n",
    "print('Mutiple Linear Regression test MSE: ', mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a86ed7-152a-409d-b9b8-8f97f4f7ccea",
   "metadata": {},
   "source": [
    "## Task 4.4\n",
    "\n",
    "Repeat Task 4.3, but this time train your model on the unscaled data. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745c4e63-0c4e-4397-9f28-cc6d332a7174",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "coefficients = model.coef_\n",
    "mse = mean_squared_error(y_test, model.predict(X_test))\n",
    "\n",
    "print('Multiple Linear Regression model coefficients: ', coefficients)\n",
    "print('Mutiple Linear Regression test MSE: ', mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d49317-a1ab-4dc2-a3df-83c056c1c8a9",
   "metadata": {},
   "source": [
    "**Observation**: The model trained on the unscaled data is equivalent to the model trained on the scaled data as can be seen by comparing the two model's test MSE which are identical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e9e9d4-71f0-4224-a6ac-08bc711d1315",
   "metadata": {},
   "source": [
    "## Task 4.5\n",
    "\n",
    "Next, we implement Lasso regression using `sklearn.linear_model.Lasso` (see [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html)). \n",
    "\n",
    "In the following, train a Lasso model on the scaled training data using the regularization parameter $\\lambda = 1$. Note that $\\lambda$ is set by specifying the argument `alpha` in `sklearn.linear_model.Lasso`.\n",
    "\n",
    "Compute the model coefficients (using the `coef_` attribute of the trained model) and the mean squared error on the test data (using the function `sklearn.metrics.mean_squared_error()` (see [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html))).\n",
    "\n",
    "*Note*: The design matrix and the results vector are passed only as arguments to the `fit()` method for `sklearn` models. This is different than for `statsmodels` where we passed the data already at the stage of initializing the model. Additionally, the order in which the design matrix and the results vector are passed to a `sklearn`-model is swapped compared to `statsmodels`!\n",
    "\n",
    "Also note: for linear models in `sklearn` we do not need to manually create an `intercept` column as we can specify if we want an intercept to be included using the `fit_intercept` argument when initializing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f97fb5-5637-4b2b-b2d7-7884dc37e43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "alpha = 1\n",
    "model = Lasso(alpha = alpha)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "coefficients = model.coef_\n",
    "mse = mean_squared_error(y_test, model.predict(X_test_scaled))\n",
    "\n",
    "print('Model coefficients: ', coefficients)\n",
    "print('Lasso test MSE for alpha = 1: ', mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9808cd-f889-4442-bcc9-f5c24ac65f2e",
   "metadata": {},
   "source": [
    "## Task 4.6\n",
    "\n",
    "For values of $\\lambda$ varying from 0.01 to 2 in steps of 0.01 train Lasso models and compute the model coefficients and the model test MSEs. For each new value of $\\lambda$, append the new model coefficients and test MSEs to lists called `coefficients_Lasso` and `mses`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899ef5a0-4408-4c23-963f-0673bd5a2d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "coefficients_Lasso = []\n",
    "mses = []\n",
    "alphas = np.arange(0.01, 2, 0.01)\n",
    "\n",
    "for a in alphas:\n",
    "    model = Lasso(alpha=a)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    coefficients_Lasso.append(model.coef_)\n",
    "    mses.append(mean_squared_error(y_test, model.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10999094-b907-4daf-8ce5-cdff6b982359",
   "metadata": {},
   "source": [
    "Run the two cells below to visualize your coefficients and your MSEs for the different $\\lambda$ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43df21c5-fa9c-42c1-af6c-ece09c0800e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = [\"x\" + str(v) for v in np.arange(1, 36, 1)]\n",
    "\n",
    "coefs_df = pd.DataFrame(coefficients_Lasso, columns=col_names)\n",
    "\n",
    "coefs_df[\"lambda\"] = alphas\n",
    "coefs_long = pd.melt(coefs_df, id_vars=[\"lambda\"], value_vars=col_names)\n",
    "\n",
    "fig = px.line(coefs_long, x=\"lambda\", y=\"value\", color=\"variable\", log_x=True)\n",
    "fig.update_layout(\n",
    "    showlegend=False, width=1000, height=500, yaxis_title=\"Coefficient\",\n",
    "    xaxis_title=\"Lambda\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e795a52-82a8-4fef-adc0-4c59f004b16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(x=alphas, y=mses,\n",
    "        labels={\"x\": \"Lambda\", \"y\": \"MSE\"},\n",
    "        width=700, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd91e1cd-b658-4f6c-8c0a-920b1a402af8",
   "metadata": {},
   "source": [
    "## Task 4.7\n",
    "\n",
    "Repeat the steps from Task 4.6, this time using Ridge regression [`sklearn.linear_model.Ridge`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge) using a parameter $\\lambda$ which varies from $1$ to $3000$ in steps of $25$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be45bc6-f9a3-4781-895f-ebd3f732d935",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "coefficients_Ridge = []\n",
    "alphasR = np.arange(1, 3000, 25)\n",
    "mses = []\n",
    "\n",
    "for a in alphasR:\n",
    "    model = Ridge(alpha=a)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    coefficients_Ridge.append(model.coef_)\n",
    "    mses.append(mean_squared_error(y_test, model.predict(X_test_scaled)))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9980b970-8608-4e3c-8823-b924c7a0985f",
   "metadata": {},
   "source": [
    "Run the two cells below to visualize the coefficients and the test score for the different $\\lambda$ parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1520b04-bde7-449d-bdc4-daee03a923d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients_Ridge = np.squeeze(coefficients_Ridge)\n",
    "\n",
    "col_names = [\"x\" + str(v) for v in np.arange(1, 36, 1)]\n",
    "\n",
    "coefsR_df = pd.DataFrame(coefficients_Ridge, columns=col_names)\n",
    "coefsR_df[\"lambda\"] = alphasR\n",
    "\n",
    "coefsR_long = pd.melt(coefsR_df, id_vars=[\"lambda\"], value_vars=col_names)\n",
    "\n",
    "fig = px.line(coefsR_long, x=\"lambda\", y=\"value\", color=\"variable\", log_x=True)\n",
    "fig.update_layout(\n",
    "    showlegend=False, width=1000, height=500, \n",
    "    yaxis_title=\"Coefficient\", xaxis_title=\"Lambda\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0febfc8d-800a-46cd-b032-67f700734c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(x=alphasR, y=mses,\n",
    "        labels={\"x\": \"Lambda\", \"y\": \"MSE\"},\n",
    "        width=700, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4fcdd3-d754-4e72-8f74-8e1542be4a9a",
   "metadata": {},
   "source": [
    "## Task 4.8\n",
    "\n",
    "Now we use $10$-fold cross validation to compare the estimated test MSE of OLS multiple linear regression, Lasso regression and Ridge regression.\n",
    "To do so, follow the steps outlined below:\n",
    "- Initialize a `KFold` cross-validator (see [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html)). Make sure to set a random state so that the same folds are used for all models. Also make sure that the data is shuffled.\n",
    "- With this cross-validator, compute the cross validation scores for the regular OLS model. Since in this part we stay completely within `sklearn` and do not use `statsmodels`, there is no need for using `sklearn_sm`. Make sure to specify the appropriate scorer using the `socring` parameter.\n",
    "- For `Lasso` and `Ridge` we need to define pass the model in the form of a pipeline to `cross_validate` to make sure that the standardization is carried out on each of the folds separately. For this, use the function `sklearn.pipeline.make_pipeline` (see [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html) or [here](https://scikit-learn.org/stable/modules/compose.html) for more details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1695d53-2b31-4b06-aace-f10786f7678b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val = KFold(n_splits = 10,\n",
    "                 shuffle = True,\n",
    "                 random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da72d17c-5273-49e7-9404-0754952e293a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine OLS cross validation score\n",
    "model = LinearRegression()\n",
    "cv_results = cross_validate(model,\n",
    "                           X,\n",
    "                           y,\n",
    "                           cv = cross_val,\n",
    "                           scoring = 'neg_mean_squared_error')\n",
    "cv_err_OLS = -np.mean(cv_results['test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad22636-a37d-4807-8700-9c0344945edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine Lasso cross validation scores\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "alphas_L = np.arange(0.01, 2, 0.01)\n",
    "n = len(alphas_L)\n",
    "cv_err_L = np.zeros(len(alphas_L))\n",
    "\n",
    "for i in range(n):\n",
    "    a = alphas_L[i]\n",
    "    model = make_pipeline(StandardScaler(), Lasso(alpha=a))\n",
    "    cv_results = cross_validate(model,\n",
    "                               X,\n",
    "                               y,\n",
    "                               cv = cross_val,\n",
    "                               scoring = 'neg_mean_squared_error')\n",
    "    cv_err_L[i] = - np.mean(cv_results['test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28512af-7918-4fb1-aa65-200e18cd6f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine Ridge cross validation scores\n",
    "alphas_R = np.arange(1, 3000, 25)\n",
    "n = len(alphas_R)\n",
    "cv_err_R = np.zeros(len(alphas_R))\n",
    "\n",
    "for i in range(n):\n",
    "    a = alphas_R[i]\n",
    "    model = make_pipeline(StandardScaler(), Ridge(alpha=a))\n",
    "    cv_results = cross_validate(model,\n",
    "                               X,\n",
    "                               y,\n",
    "                               cv = cross_val,\n",
    "                               scoring = 'neg_mean_squared_error')\n",
    "    cv_err_R[i] = - np.mean(cv_results['test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503541a7-abcf-4f0f-bb6b-d04189f4af13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Cross validation score OLS: ', cv_err_OLS)\n",
    "print('Best cross validation score Lasso: ', min(cv_err_L), ' (for parameter alpha = ', \n",
    "      alphas_L[np.argmin(cv_err_L)],')')\n",
    "print('Best cross validation score Ridge: ', min(cv_err_R), ' (for parameter alpha = ', \n",
    "      alphas_R[np.argmin(cv_err_R)],')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fec03d-2b0b-4d55-accc-09476a725dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_err_R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaade1d-6da2-4aa3-bdfb-439e61ab0752",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
